{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 15 – Intro to Neural Networks with NumPy\n",
    "\n",
    "This notebook condenses `numpy_advanced_intro_to_neural_nets.md` into a structured workspace. Work through each exercise in order: first understand the idea, then implement it, and finally reflect on the takeaways."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workflow\n",
    "1. **Research (15–30 min):** clarify the concept, its purpose, the math intuition, and look for visuals.\n",
    "2. **Implementation (20–30 min):** write the code only after you know what you are doing; comment on intent and check matrix shapes.\n",
    "3. **Reflection (10–15 min):** record what you learned, open questions, and any \"aha\" moments.\n",
    "\n",
    "Recommended resources: 3Blue1Brown for intuition, Papers with Code for practice, and the NumPy docs for vectorization tips. Avoid blind copy/paste, iterate in small steps, and validate each change."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise I – Single ReLU Neuron\n",
    "- Implement `NeuronSimplu` with random weights/bias (seeded for reproducibility).\n",
    "- Add `relu(x) = max(0, x)` and use it inside `forward` to compute `ReLU(weights · inputs + bias)`.\n",
    "- Print the parameters and output for a sample input, plus any helpful shape checks.\n",
    "- In markdown, explain why ReLU adds non-linearity, mention pitfalls such as the dying-ReLU issue, and describe the intuition behind weights and bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class SimpleNeuron:\n",
    "    def __init__(self):\n",
    "        random.seed(42)\n",
    "\n",
    "        # weights representing influence of last layer on neuron\n",
    "        self.weights = [random.uniform(0, 1) for _ in range(4)]\n",
    "        self.bias = 0\n",
    "\n",
    "    def relu(self, x):\n",
    "        return max(0, x)\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.weights @ x + self.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "93e47263",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred: [-0.67112489]\n",
      "true: -0.6711248926388986\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "class Layer:\n",
    "    def __init__(self, n_in, n_out, use_relu=True):\n",
    "        random.seed(42)\n",
    "\n",
    "        self.W = np.random.uniform(-1, 1, (n_out, n_in))\n",
    "        self.b = np.random.uniform(-1, 1, (n_out,))\n",
    "        self.use_relu = use_relu\n",
    "        self.a = np.zeros(n_out)\n",
    "\n",
    "    def relu(self, z):\n",
    "        return np.maximum(0, z)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        z = self.W @ x + self.b\n",
    "        self.a = self.relu(z) if self.use_relu else z\n",
    "        return self.a\n",
    "\n",
    "class Layers:\n",
    "    def __init__(self):\n",
    "        self.layers: list[Layer] = []\n",
    "\n",
    "    def add(self, n_in, n_out, use_relu=True):\n",
    "        self.layers.append(Layer(n_in, n_out, use_relu))\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "\n",
    "# Test\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def make_data(n=1000):\n",
    "    X = np.random.uniform(-1, 1, (n, 2))\n",
    "    y = X[:, 0] + X[:, 1]\n",
    "    return X, y\n",
    "\n",
    "def train(net, X, y, lr=0.01, epochs=200):\n",
    "    for _ in range(epochs):\n",
    "        for xi, yi in zip(X, y):\n",
    "            out = net.forward(xi)\n",
    "\n",
    "            error = out - yi   # (1,)\n",
    "\n",
    "            layer = net.layers[-1]\n",
    "\n",
    "            # derivative of activation\n",
    "            if layer.use_relu:\n",
    "                dz = (layer.a > 0).astype(float) * error\n",
    "            else:\n",
    "                dz = error  # linear output\n",
    "\n",
    "            # update\n",
    "            layer.W -= lr * dz[:, None] * xi[None, :]\n",
    "            layer.b -= lr * dz\n",
    "\n",
    "net = Layers()\n",
    "net.add(2, 1, use_relu=False)\n",
    "\n",
    "X, y = make_data()\n",
    "# print([round(float(x), 5) for x in y])\n",
    "train(net, X, y)\n",
    "\n",
    "a = random.uniform(-1, 1)\n",
    "b = random.uniform(-1, 1)\n",
    "\n",
    "test = np.array([a, b])\n",
    "print(\"pred:\", net.forward(test))\n",
    "print(\"true:\", a + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f18b5f",
   "metadata": {},
   "source": [
    "# Simple Network Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2784e868",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred: [0.7]\n",
      "true: 0.7\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def make_data(n=1000):\n",
    "    X = np.random.uniform(-1, 1, (n, 2))\n",
    "    y = X[:, 0] + X[:, 1]\n",
    "    return X, y\n",
    "\n",
    "def train(net: Layers, X, y, lr=0.01, epochs=200):\n",
    "    for _ in range(epochs):\n",
    "        for xi, yi in zip(X, y):\n",
    "            # forward\n",
    "            out = net.forward(xi)\n",
    "\n",
    "            # compute error (simple squared loss)\n",
    "            error = out - yi           # shape (1,)\n",
    "            \n",
    "            # gradient w.r.t z (ReLU derivative)\n",
    "            dz = (net.layers[-1].a > 0).astype(float) * error\n",
    "\n",
    "            # update weights: W -= lr * dz * x\n",
    "            layer = net.layers[-1]\n",
    "            layer.W -= lr * dz[:, None] * xi[None, :]\n",
    "            layer.b -= lr * dz\n",
    "\n",
    "net = Layers()\n",
    "net.add(2, 1)\n",
    "\n",
    "X, y = make_data()\n",
    "# print([round(float(x), 5) for x in y])\n",
    "train(net, X, y)\n",
    "\n",
    "test = np.array([0.3, 0.4])\n",
    "print(\"pred:\", net.forward(test))\n",
    "print(\"true:\", 0.3 + 0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise II – Sigmoid on Random Data\n",
    "- Generate 100 random real values, apply a vectorized sigmoid, and store both the raw and transformed arrays.\n",
    "- Report mean/min/max before and after sigmoid and compare the histograms to show how the function squeezes values into (0, 1).\n",
    "- Document where sigmoid shines (binary classification, probability outputs), when it struggles (vanishing gradients), and how it differs from tanh or softmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise III – Two-Layer Vectorized Network\n",
    "- Build `ReteasNeuronala` with dense layers input→hidden→output, ReLU in the hidden layer, sigmoid in the output layer.\n",
    "- Keep everything vectorized: `Z1 = X · W1 + b1`, `A1 = ReLU(Z1)`, `Z2 = A1 · W2 + b2`, `A2 = sigmoid(Z2)`, `prezice()` returns `(A2 > 0.5).astype(int)`.\n",
    "- Test with synthetic data, print parameter and activation shapes, and prove the batch processing works.\n",
    "- Note current limits (no training/backprop yet, hidden size is a manual choice) and how you would extend this skeleton to deeper nets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation & Good Practices\n",
    "- **Correctness (40%)** – math functions and matrix ops behave as intended.\n",
    "- **Vectorization (30%)** – NumPy handles the heavy lifting; no unnecessary loops.\n",
    "- **Clarity (20%)** – meaningful names plus comments that explain intent.\n",
    "- **Testing (10%)** – run sample inputs, print shapes/results, and check reproducibility.\n",
    "\n",
    "Always log new questions, seed randomness when needed, and use shape prints to debug quickly."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
